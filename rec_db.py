import psycopg2
from datetime import datetime, timezone


def analyze_pg_settings(conn, ram_gb=None):
    recommendations = []
    with conn.cursor() as cur:
        cur.execute("""
            SELECT name, setting, unit, context
            FROM pg_settings
            WHERE name IN (
                'work_mem',
                'shared_buffers',
                'effective_cache_size',
                'maintenance_work_mem',
                'autovacuum',
                'autovacuum_vacuum_scale_factor',
                'autovacuum_naptime',
                'max_parallel_workers_per_gather'
            )
        """)
        params = {name: (setting, unit, context) for name, setting, unit, context in cur.fetchall()}

    # shared buffers
    if "shared_buffers" in params:
        setting, unit, _ = params["shared_buffers"]
        sb = int(setting)
        sb_bytes = sb * 8 * 1024 if unit == "8kB" else sb
        if ram_gb:
            ram_bytes = ram_gb * (1024**3)
            ratio = sb_bytes / ram_bytes
            if ratio < 0.25:
                recommendations.append({
                    "type": "parameter",
                    "priority": "medium",
                    "message": f"shared_buffers = {round(ratio*100,1)}% RAM ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 25‚Äì40% RAM"
                })

    # work_mem
    if "work_mem" in params:
        setting, unit, _ = params["work_mem"]
        wm_kb = int(setting)
        if wm_kb < 4096:  # <4MB
            recommendations.append({
                "type": "parameter",
                "priority": "medium",
                "message": f"work_mem = {wm_kb/1024:.1f}MB ‚Äî —É–≤–µ–ª–∏—á–∏—Ç—å (8‚Äì64MB) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–æ–∫/–¥–∂–æ–∏–Ω–æ–≤"
            })

    # maintenance work mem
    if "maintenance_work_mem" in params:
        setting, unit, _ = params["maintenance_work_mem"]
        mw_kb = int(setting)
        if mw_kb < 65536:  # <64MB
            recommendations.append({
                "type": "parameter",
                "priority": "low",
                "message": f"maintenance_work_mem = {mw_kb/1024:.1f}MB ‚Äî —É–≤–µ–ª–∏—á–∏—Ç—å (128‚Äì512MB) –¥–ª—è VACUUM/CREATE INDEX"
            })

    # autovacuum
    if "autovacuum" in params:
        setting, _, _ = params["autovacuum"]
        if setting.lower() in ("off", "false", "0"):
            recommendations.append({
                "type": "parameter",
                "priority": "high",
                "message": "autovacuum –≤—ã–∫–ª—é—á–µ–Ω ‚Äî –≤–∫–ª—é—á–∏—Ç–µ, –∏–Ω–∞—á–µ —Ç–∞–±–ª–∏—Ü—ã –±—É–¥—É—Ç —Ä–∞–∑—Ä–∞—Å—Ç–∞—Ç—å—Å—è"
            })

    return recommendations


def analyze_autovacuum(conn, dead_tup_threshold=0.2, analyze_threshold=0.1, max_autovacuum_age_hours=24):
    results = []
    with conn.cursor() as cur:
        cur.execute("""
            SELECT relid, schemaname, relname,
                   n_live_tup, n_dead_tup, n_mod_since_analyze,
                   last_autovacuum, last_analyze
            FROM pg_stat_all_tables
            WHERE schemaname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
        """)
        tables = cur.fetchall()

        for relid, schema, table, live, dead, mod_since_analyze, last_auto, last_analyze in tables:
            total = (live or 0) + (dead or 0)
            dead_pct = (dead / total) if total > 0 else 0
            analyze_pct = (mod_since_analyze / total) if total > 0 else 0

            recommendations = []
            now = datetime.now(timezone.utc)

            if dead_pct > dead_tup_threshold:
                recommendations.append(
                    f"‚ö†Ô∏è {round(dead_pct*100, 1)}% dead tuples ‚Äî –∑–∞–ø—É—Å—Ç–∏—Ç–µ VACUUM –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ autovacuum_vacuum_scale_factor"
                )
            if last_auto and (now - last_auto).total_seconds() > max_autovacuum_age_hours * 3600:
                recommendations.append(
                    f"‚ö†Ô∏è –ê–≤—Ç–æ–≤–∞–∫—É—É–º –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª—Å—è {round((now - last_auto).total_seconds()/3600, 1)} —á ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ autovacuum_naptime"
                )
            if analyze_pct > analyze_threshold:
                recommendations.append(
                    f"‚ö†Ô∏è –° –º–æ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ ANALYZE –∏–∑–º–µ–Ω–µ–Ω–æ {round(analyze_pct*100, 1)}% —Å—Ç—Ä–æ–∫ ‚Äî –≤—ã–ø–æ–ª–Ω–∏—Ç–µ ANALYZE"
                )

            # ‚úÖ –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º recommendations, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—É—Å—Ç–æ
            results.append({
                "schema": schema,
                "table": table,
                "dead_pct": round(dead_pct * 100, 1),
                "analyze_pct": round(analyze_pct * 100, 1),
                "last_autovacuum": last_auto,
                "last_analyze": last_analyze,
                "recommendations": recommendations
            })

    return results


def analyze_table_structure(conn, partition_threshold_mb=10_000):
    """–°–æ–≤–µ—Ç—ã –ø–æ —Å–µ–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –¥–µ—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    recommendations = []
    with conn.cursor() as cur:
        cur.execute("""
            SELECT relname,
                   pg_total_relation_size(relid) AS total_size
            FROM pg_catalog.pg_statio_user_tables;
        """)
        for table, total_size in cur.fetchall():
            size_mb = total_size / (1024 * 1024)
            if size_mb > partition_threshold_mb:
                recommendations.append({
                    "type": "table",
                    "table": table,
                    "priority": "medium",
                    "message": f"–¢–∞–±–ª–∏—Ü–∞ {table} > {partition_threshold_mb}MB ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å–µ–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –¥–∞—Ç–µ –∏–ª–∏ –∫–ª—é—á—É"
                })
    return recommendations

def analyze_table_bloat(conn, bloat_threshold=0.2):
    """
    –ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü (bloat). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ VACUUM FULL / CLUSTER.
    """
    results = []
    with conn.cursor() as cur:
        # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pgstattuple, –Ω–æ –æ–Ω–∞ –Ω–µ –≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–µ–Ω–∞
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã ‚Äî –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ pg_stat_all_tables
        cur.execute("""
            SELECT
                schemaname,
                relname,
                n_live_tup,
                n_dead_tup,
                pg_total_relation_size(relid) AS total_bytes
            FROM pg_stat_all_tables
            WHERE schemaname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
        """)
        tables = cur.fetchall()

        for schema, table, live, dead, total_bytes in tables:
            total_rows = (live or 0) + (dead or 0)
            dead_pct = (dead / total_rows) if total_rows > 0 else 0

            recommendations = []
            if dead_pct > bloat_threshold:
                recommendations.append(
                    f"‚ö†Ô∏è Bloat ‚âà {round(dead_pct * 100, 1)}% ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ VACUUM FULL –∏–ª–∏ CLUSTER"
                )

            results.append({
                "schema": schema,
                "table": table,
                "bloat_pct": round(dead_pct * 100, 1),
                "size_mb": round(total_bytes / (1024 * 1024), 2),
                "recommendations": recommendations
            })

    return results

def analyze_index_usage(conn, unused_threshold=0.05):
    """
    –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ schema, table, index, recommendations.
    """
    results = []
    with conn.cursor() as cur:
        cur.execute("""
            SELECT
                schemaname,
                relname AS table_name,
                indexrelname AS index_name,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch,
                pg_relation_size(indexrelid) AS index_size
            FROM pg_stat_all_indexes
            WHERE schemaname NOT IN ('pg_catalog', 'information_schema', 'pg_toast')
        """)
        indexes = cur.fetchall()

        for schema, table, index, idx_scan, tup_read, tup_fetch, size in indexes:
            recommendations = []

            if idx_scan == 0:
                recommendations.append("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞")
            elif idx_scan < unused_threshold * tup_read:
                recommendations.append("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å —Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ, –Ω—É–∂–µ–Ω –ª–∏ –æ–Ω")

            results.append({
                "schema": schema,
                "table": table,
                "index": index,
                "size_mb": round(size / (1024 * 1024), 2),
                "idx_scan": idx_scan,
                "idx_tup_read": tup_read,
                "idx_tup_fetch": tup_fetch,
                "recommendations": recommendations
            })

    return results


def health_check(conn, ram_gb=None, verbose=True):
    results = []

    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ PostgreSQL
    pg_settings = analyze_pg_settings(conn, ram_gb=ram_gb)
    if verbose:
        print("\n‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ PostgreSQL:")
        for r in pg_settings:
            print(f"  ‚Ä¢ {r['recommendation']}")
    results.extend(pg_settings)

    # 2. –ê–≤—Ç–æ–≤–∞–∫—É—É–º
    autovac = analyze_autovacuum(conn)
    if verbose:
        print("\nüßπ –ê–≤—Ç–æ–≤–∞–∫—É—É–º –∏ ANALYZE:")
        if not autovac:
            print("   –ü—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        for r in autovac:
            print(f"  ‚Ä¢ {r['schema']}.{r['table']}:")
            for rec in r["recommendations"]:
                print(f"    - {rec}")
    results.extend(autovac)

    # 3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü
    tbl_struct = analyze_table_structure(conn)
    if verbose:
        print("\nüìê –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü:")
        if not tbl_struct:
            print("   –Ø–≤–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        for r in tbl_struct:
            print(f"  ‚Ä¢ {r['schema']}.{r['table']}:")
            for rec in r["recommendations"]:
                print(f"    - {rec}")
    results.extend(tbl_struct)

    # 4. –§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü
    tbl_bloat = analyze_table_bloat(conn)
    if verbose:
        print("\nüìä –§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü (bloat):")
        if not tbl_bloat:
            print("  ‚úÖ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ bloating –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        for r in tbl_bloat:
            print(f"  ‚Ä¢ {r['schema']}.{r['table']}: bloat ‚âà {r['bloat_pct']}%")
            for rec in r["recommendations"]:
                print(f"    - {rec}")
    results.extend(tbl_bloat)

    # 5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
    idx_usage = analyze_index_usage(conn)
    if verbose:
        print("\nüîé –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤:")
        if not idx_usage:
            print("   –ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        for r in idx_usage:
            print(f"  ‚Ä¢ {r['schema']}.{r['table']}.{r['index']}:")
            for rec in r["recommendations"]:
                print(f"    - {rec}")
    results.extend(idx_usage)

    return results



if __name__ == "__main__":
    conn = psycopg2.connect(
        host="localhost", dbname="dvdrental", user="postgres", password=""
    )
    try:
        recs = health_check(conn, ram_gb=16)
        for r in recs:
            prefix = f"[{r.get('priority','info').upper()}]"
            print(f"{prefix} {r['message']}")
    finally:
        conn.close()
